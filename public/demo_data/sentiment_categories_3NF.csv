CategoryID,Category,Reason,Level,Description,Shows up as
1,The Intrusive AI,AI is too Autonomous,Personal Workflow Preferences,"This category involves light but persistent friction when AI disrupts users’ preferred ways of working. People feel overridden or second-guessed by systems that act without request or context. The discomfort is mild—often expressed as a preference for manual control—but reflects an emotional resistance to AI taking over familiar tasks. It’s not about deep fear, but annoyance with automation that inserts itself too readily or confidently into personal workflows, reducing the sense of autonomy.",People disable auto-features and revert to manual steps * Complaints such as 'I didn’t ask it to do that' * Workarounds to avoid prompts or auto-edits * Preference for offline or static tools * Ignoring AI suggestions even when useful
2,The Unadaptive AI,AI is too Inflexible,Personal Workflow Preferences,"This category captures resistance to AI systems that can’t or won’t adapt to individual work habits. The discomfort arises when people perceive AI as one-size-fits-all—forcing sameness, missing personal context, or repeating errors. Even when technically functional, these tools feel misaligned with how users operate, creating irritation. The issue is not performance but personalization: users want tools that learn and evolve, not ones that lock them into rigid flows that overlook nuance or change.",Feedback like 'It never learns my way' * Using custom spreadsheets/templates instead of AI * Repetitive corrections without improvement * Avoiding smart modes; sticking to basics * Frequent requests for per-user customization
3,The Uncaring AI,AI is Emotionless,Personal Workflow Preferences,"This category reflects subtle discomfort when AI interactions lack human tone or emotional awareness. People feel unseen or dismissed, especially in tasks where memory, tone, or empathy matter. Even in small moments, users notice when AI treats them impersonally—forgetting past input, giving generic responses, or missing cues of emotion. These systems don’t need to “feel,” but their absence of care makes them feel inferior to human alternatives, especially in situations where warmth or acknowledgment is expected.","Complaints that 'the tone is off' in drafts * Preferring colleagues for sensitive tasks * Adding extra notes/emojis to soften outputs * Annoyance when AI forgets past inputs * Short, transactional use of AI with little reliance"
4,The Confusing AI,AI is too Opaque,Personal Workflow Preferences,"This category highlights frustration when AI systems are hard to understand or explain. People feel alienated by unclear outputs, too much data, or hidden processes. Even if technically accurate, these tools create unease by obscuring their logic or presenting too much without clarity. Users may resist adopting AI that leaves them feeling unsure about what it’s doing, why it acts as it does, or how it draws conclusions—especially when transparency is key to trust.",Questions like 'How did it get that?' * Reluctance to act without clear reasoning * Skipping dashboards seen as unclear * Preference for simple rules over opaque advice * Requests for plain-language explanations
5,The Aloof AI,People Prefer Human Interaction,Personal Workflow Preferences,"This category reflects mild discomfort when AI lacks the warmth or social texture of human interaction. Even when functional, AI tools can feel cold or emotionally disconnected—failing to replicate the rapport, tone, or care found in human contact. This creates a sense of detachment, especially in tasks where users seek acknowledgment, personality, or shared understanding. It’s not that AI performs poorly, but that it feels distant—evoking avoidance when users want more than just utility.",Asking 'Can I just talk to a person?' * Choosing human help channels over AI * Very short AI interactions * Walking over to a colleague instead * Avoiding AI when rapport matters
6,The Redefining AI,AI is too Autonomous,Collaboration & Role Adjustments,"This category reflects tension when AI changes how tasks and authority are distributed within teams. People feel roles are redefined without consent—either offloaded to AI or reorganized in ways that erode clarity and trust. It introduces subtle power shifts, with some team members aligning closely with AI tools while others resist, creating division. The discomfort arises not from the AI’s capability, but from its silent reshaping of how people work together, make decisions, and define their contributions.",Asking 'Who owns this decision now?' * Informal reassignments to regain control * Using shadow spreadsheets to check outputs * Tension between AI adopters and holdouts * Governance debates about accountability
7,The Forcing AI,AI is too Inflexible,Collaboration & Role Adjustments,"This category involves frustration when AI enforces rigid standards or group behaviors that reduce flexibility and individual input. People feel pressured to conform to structures that may not fit their context or priorities. The discomfort builds as AI normalizes uniformity—making team processes less dynamic, stifling creative collaboration, or exaggerating patterns that don’t serve everyone. Resistance grows when AI locks groups into narrow ways of working that can’t evolve with changing needs or insights.",Complaints like 'We have to do it the tool’s way' * Escalations for exceptions AI won’t handle * Creative work reduced to fit AI templates * Workarounds to preserve flexibility * Frustration with rigid project rules
8,The Distant AI,AI is Emotionless,Collaboration & Role Adjustments,"This category reflects unease when AI doesn’t “show up” emotionally in team dynamics. People find it hard to trust or engage with AI that feels impassive, especially in roles that require care, empathy, or understanding. The discomfort emerges in tasks where subtle cues matter—where human collaborators bring tone, intent, or shared meaning. AI’s coldness can feel like betrayal in sensitive moments, undermining its presence as a collaborator or tool in socially complex situations.",Remarks like 'It doesn’t get the client/context' * Pulling AI out of sensitive meetings * Human rewrites for tone and nuance * Over-explaining briefs to avoid misfires * Avoiding AI for conflict or escalations
9,The Obscuring AI,AI is too Opaque,Collaboration & Role Adjustments,"This category captures mistrust when AI clouds team understanding or reshapes how contributions are recognized. People worry that important details are hidden, misinterpreted, or reduced by the AI’s logic. It creates discomfort when group efforts are devalued or misunderstood, especially if AI filters become the gatekeepers of credit, context, or evaluation. Resistance grows when team members feel disempowered or unable to see or challenge how AI influences shared outcomes.",Using 'the system decided' without reasoning * Disputes over recognition or credit * Requests for audit trails * Suspicion of lost nuance * Calls for opt-outs in critical work
10,The Separating AI,People Prefer Human Interaction,Collaboration & Role Adjustments,"This category involves discomfort when AI mediates or replaces human connection in teams. People notice growing gaps—fewer casual interactions, less spontaneous collaboration, or reliance on structured digital systems. The AI may work, but the social glue weakens. Over time, the absence of direct contact erodes trust and belonging. Resistance emerges as people seek to preserve emotional connection and informal communication, which are hard to replicate in AI-shaped environments.",Fewer spontaneous chats among colleagues * Comments like 'We don’t talk anymore' * Extra team-building efforts * Less video engagement during AI mediation * Dependence on structured handoffs
11,The Unjust AI,AI is too Autonomous,Professional Trust & Fairness Issues,"This category represents serious discomfort with fairness and justice in AI decisions. People worry about bias, lack of accountability, or decisions made without oversight. The fear isn’t just being replaced, but being unfairly treated or evaluated by systems they cannot question. There’s a strong sense of moral tension—especially when AI influences high-stakes outcomes like promotions, reviews, or access to resources. Resistance here is principled, not just practical.",Escalations about bias in decisions * Refusal to accept AI-driven scores * Creating appeal or override processes * Demanding data access for fairness checks * Pausing adoption until fairness confirmed
12,The Constricting AI,AI is too Inflexible,Professional Trust & Fairness Issues,"This category reflects resistance when AI restricts professional identity or options. People feel boxed into roles or judged by fixed models that don’t capture their uniqueness. The discomfort comes from systems that limit how people grow, challenge norms, or express diverse approaches. It also reflects concern that AI favors certain types of people or behaviors—unintentionally reinforcing bias or inequality under the guise of standardization or optimization.",Complaints like 'This rubric doesn’t fit my work' * Edge cases rejected repeatedly * Portfolios not mapping to fixed fields * Perceptions of favoritism or exclusion * Seeking alternative venues to showcase work
13,The Callous AI,AI is Emotionless,Professional Trust & Fairness Issues,"This category involves deeper resistance to AI perceived as indifferent to ethical outcomes or human impact. People feel emotionally and morally alienated when AI makes decisions that harm others, overlook consequences, or optimize in cold, utilitarian ways. The discomfort intensifies when AI seems to sidestep social responsibility or enables outcomes that humans would avoid. This form of resistance speaks to values—challenging the legitimacy of AI in domains requiring compassion or ethical judgment.",Comments like 'This outcome harms people' * Extra ethics reviews for AI use * Refusing AI for human-impact decisions * Demanding human sign-off for sensitive tasks * Reframing goals to include human impact
14,The Hidden AI,AI is too Opaque,Professional Trust & Fairness Issues,"This category captures anxiety over decisions made by invisible or unchallengeable systems. People feel excluded from knowing how outcomes are reached or from influencing them. It reflects a sense of vulnerability—especially when stakes are high, and AI processes are shielded by complexity, jargon, or design. Trust erodes when people suspect that vital judgments are being made in ways that cannot be explained or audited.",Questions like 'Who configured this and why?' * Compliance demanding model documentation * Disputes about access for checks * Freezing use until clarity improves * Pressure to simplify technical language
15,The Usurping AI,People Prefer Human Interaction,Professional Trust & Fairness Issues,"This category describes discomfort when AI replaces roles that carry meaning, identity, or authority. People resist systems that offload judgment, reduce their influence, or substitute their expertise. Even if the outcomes are efficient, the emotional loss is significant. It’s about being removed from processes that once defined one’s value or control. The result is often quiet withdrawal or open pushback against being replaced by faceless logic.",Complaints like 'My judgment no longer matters' * Withdrawing from AI-automated steps * Frustration at losing signature tasks * Concerns about identity loss * Pushback to restore human involvement
16,The Threatening AI,AI is too Autonomous,Career Security & Job Redefinition Anxiety,"This category captures acute fear about personal job loss or radical role change. People worry that AI won’t just assist them—it will replace them. The discomfort is tied to livelihood and identity, especially when roles feel directly targeted by automation. Resistance here is emotional and defensive, rooted in anxiety about future relevance, skills becoming obsolete, or being phased out entirely. It’s not a dislike of the technology, but a profound fear of what it means for one’s career trajectory and security.",Rumors linking AI pilots to layoffs * Low training uptake due to replacement fear * Reluctance to document tacit knowledge * Updating CVs and job hunting * Union or staff council interventions
17,The Stagnating AI,AI is too Inflexible,Career Security & Job Redefinition Anxiety,"This category reflects fear that AI will freeze professional growth or trap people in dead-end roles. Instead of evolving with people, the AI limits change, repeating patterns or prioritizing short-term gains. Employees feel their skills are at risk of decay or irrelevance, and that opportunities to grow, shift roles, or build new capacities are shrinking. Resistance stems from the belief that AI is keeping them stagnant rather than supporting their development.",Stay interviews citing 'no room to grow' * Blocked role rotations * Skills-atrophy concerns * Shadow projects outside AI scope * Requests for human-led learning programs
18,The Devaluing AI,AI is Emotionless,Career Security & Job Redefinition Anxiety,"This category centers on the emotional harm of being overlooked or dehumanized by AI systems. People feel their contributions are undervalued—reduced to data points or judged without context. The discomfort is personal: not only are tasks being automated, but recognition, creativity, and judgment are being stripped away. Resistance arises when AI seems to treat workers as replaceable or their emotional input as irrelevant, especially in roles where acknowledgment or meaning matters.",Comments like 'No one sees my effort anymore' * Disengagement with recognition systems * Cynicism in employee surveys * Avoiding tasks where AI overshadows contributions * Burnout linked to automation narratives
19,The Unknowable AI,AI is too Opaque,Career Security & Job Redefinition Anxiety,"This category reflects anxiety when career-impacting AI decisions are hard to understand or trace. People feel watched but not understood, evaluated by metrics they can’t see or influence. It evokes helplessness—when roles are redefined, evaluated, or removed based on hidden logic. The discomfort becomes existential when employees don’t know how to adapt, improve, or even respond. It’s not just opacity—it’s being unable to navigate or control one’s own future.",Comments like 'I don’t know how to improve my rating' * Fear of hidden tracking * Appeals of AI-driven performance decisions * Avoiding internal moves tied to opaque scoring * Demanding transparency on evaluation criteria
20,The Replacing AI,People Prefer Human Interaction,Career Security & Job Redefinition Anxiety,"This category involves fear that AI will strip roles of human contact, making people irrelevant in their own field. It’s not only about automation, but about being cut off from the social and emotional aspects of work that give meaning and connection. People fear being submerged beneath AI layers, with less voice, visibility, or purpose. Even when jobs remain, the role’s essence feels eroded—pushing workers toward disengagement or despair.",Customers preferring human agents * Employees feeling their role is hollowed out * Increased sick days or attrition * Refusal to push self-service AI * Escalations to restore human touch
21,The Uncontrolled AI,AI is too Autonomous,Organizational Stability at Risk,"This category reflects fear that autonomous AI could destabilize core operations. People worry that these systems might act unpredictably or create large-scale disruptions. The discomfort isn’t about efficiency—it’s about control slipping away. There’s anxiety over who steers the system, what unintended changes might unfold, and whether oversight can keep up. This form of resistance is often voiced at leadership levels, where systemic risk and strategic misalignment are primary concerns.",Leadership demanding kill-switches * Change freezes until safeguards in place * Mandatory checkpoints in workflows * Contingency drills for AI failures * Centralized escalation protocols
22,The Brittle AI,AI is too Inflexible,Organizational Stability at Risk,"This category signals concern that AI systems are too rigid to survive real-world variability. People fear over-dependence on tools that can’t adapt when things change. There’s worry that inflexibility will become a structural weakness—locking organizations into brittle processes or failing to accommodate exceptions, diversity, or evolving needs. Resistance here focuses on resilience, adaptability, and the capacity to respond to shocks or unexpected challenges.",Process outages due to rigid AI * Concerns about vendor lock-in * Chronic exception handling issues * Resilience drills for failures * Push to diversify or reintroduce manual backups
23,The Dehumanizing AI,AI is Emotionless,Organizational Stability at Risk,"This category captures fears that widespread AI adoption could erode core human values in the workplace. People worry that efficiency will come at the cost of meaning, connection, or care. The discomfort isn’t limited to frontline roles—leaders may resist systems that remove the soul from operations, culture, or customer experience. It’s a philosophical pushback: not just what AI does, but what it turns organizations into.",Brand feedback calling AI 'cold' or 'uncaring' * Leaders emphasizing 'humanity' publicly * Refusal to automate customer care moments * KPIs added for warmth and empathy * Investment in human-to-human service layers
24,The Risky AI,AI is too Opaque,Organizational Stability at Risk,"This category involves concern about systemic vulnerabilities introduced by opaque AI. People fear hidden risks—legal, reputational, operational—that can arise when decision-making is unclear or untraceable. The discomfort escalates when leaders realize they can’t fully explain or defend how decisions are made, especially under pressure. Trust erodes across the system when clarity and accountability are missing.",Compliance and regulator questions * Crisis simulations for AI risks * Legal demands for explainability * Insurance reviews of AI models * Delays in launches pending clarity
25,The Undermining AI,People Prefer Human Interaction,Organizational Stability at Risk,"This category reflects fear that AI could erode the social fabric of organizations. People worry that relational trust, morale, and collaboration will degrade as human roles are sidelined. The discomfort becomes structural—undermining shared purpose or the culture that holds teams together. This resistance surfaces when people see AI not as a tool, but as a force that hollows out what made the organization feel human and cohesive.",Declining trust and more siloing * Attrition in relationship-heavy roles * Employee activism for human contact * Survey results about 'loss of soul' * Strategy shifts to hybrid human-AI approaches
